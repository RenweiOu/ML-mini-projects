{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本分类任务实战\n",
    "\n",
    "- 数据集构建：影评数据集进行情感分析（分类任务）\n",
    "- 词向量模型：加载训练好的词向量或者自己训练都可以\n",
    "- 序列网络模型：训练RNN模型进行识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/SentimentAnalysis2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN模型所需数据解读：\n",
    "\n",
    "![title](./img/1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import logging\n",
    "\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载影评数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集和测试集各有25000条影评，已经经过分词处理\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "# 每个元素都是一个词的id\n",
    "# 如果想通过id找到词，需要使用get_word_index()方法拿到word2id\n",
    "print(x_train[0])\n",
    "\n",
    "# 该数据集中每一条数据都是以id=1开头，id为1表示句子的起始符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意这里得到的是word2id，如果想要得到id2word，简单的转换一下即可\n",
    "word2id = tf.keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1),\n",
       " ('and', 2),\n",
       " ('a', 3),\n",
       " ('of', 4),\n",
       " ('to', 5),\n",
       " ('is', 6),\n",
       " ('br', 7),\n",
       " ('in', 8),\n",
       " ('it', 9),\n",
       " ('i', 10)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以查看一下词频最高的几个词是什么\n",
    "sorted(list(word2id.items()), key=lambda x:x[1])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意返回的x_train中单词的id和get_word_index()方法返回的单词的id不是一一对应的，如果index_from参数设置为3，那么实际上x_train中的id是一个词原来的id再加上3，例如get_word_index()方法返回的字典中the的id是1，那么在x_train中id为4的词才对应的是the，而不是1, 也就是说在x_train中的id从4开始才是实际的单词，id为0,1,2,3都是表示一些特殊的标记，而不是实际的单词\n",
    "\n",
    "imdb.load_data()方法中默认使用1表示开始字符，2表示oov字符，即如果文本中出现了不在字典中的单词，会使用2代替，但是这些字符不会出现在get_word_index()方法返回的dict中，因此需要对get_word_index()方法返回的dict先进行一些处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 现在制作新的词ID映射表，空出来4个位置的目的是加上特殊字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<pad>', 0),\n",
       " ('<start>', 1),\n",
       " ('<unk>', 2),\n",
       " ('the', 4),\n",
       " ('and', 5),\n",
       " ('a', 6),\n",
       " ('of', 7),\n",
       " ('to', 8),\n",
       " ('is', 9),\n",
       " ('br', 10)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i+3是为了留出0,1,2,3这4个空位，用来保存一些特殊用处的字符\n",
    "# 加3是因为imdb.load_data()设置了index_from参数为3，\n",
    "# 如果修改了index_from为N，那么应该加N\n",
    "# 原来第一个词the的index是1，现在变成4\n",
    "word2id_new = {w: i + 3 for w, i in word2id.items()}\n",
    "\n",
    "# <pad>表示补齐字符，如果一些文章的长度不足，使用<pad>补齐\n",
    "word2id_new['<pad>'] = 0\n",
    "\n",
    "# <start>表示文章的起始字符\n",
    "word2id_new['<start>'] = 1\n",
    "\n",
    "# <unk>表示未出现在词典中的词，即oov(out-of-vocabulary)\n",
    "word2id_new['<unk>'] = 2\n",
    "\n",
    "# id为3的位置空出来，暂时不使用\n",
    "\n",
    "sorted(list(word2id_new.items()), key=lambda x:x[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', 'the')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交换一下就得到id2word\n",
    "id2word_new = {i: w for w, i in word2id_new.items()}\n",
    "\n",
    "id2word_new[0], id2word_new[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按文本长度进行排序\n",
    "def sort_by_len(x, y):\n",
    "    \n",
    "    # 这行代码很精妙，仔细理解一下\n",
    "    idx = sorted(range(len(x)), key=lambda i: len(x[i]))\n",
    "    \n",
    "    # 拿到排序的id后，传回原来的数据中，得到排好序的数据\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "# 现在得到整理后的数据集\n",
    "x_train, y_train = sort_by_len(x_train, y_train)\n",
    "x_test, y_test = sort_by_len(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 13, 586, 851, 14, 31, 60, 23, 2863, 2364, 314]),\n",
       "       list([1, 14, 20, 9, 394, 21, 12, 47, 49, 52, 302]),\n",
       "       list([1, 1390, 128, 2257, 723, 8965, 60, 48, 25, 28, 296, 12]),\n",
       "       list([1, 12039, 4, 12632, 127, 6, 117, 67102, 5, 6, 20, 91, 3939]),\n",
       "       list([1, 6741, 20576, 9, 321, 9, 14, 22, 29, 166, 6, 1429, 255]),\n",
       "       list([1, 196, 357, 16445, 115, 28, 13, 77, 38, 1264, 8, 67, 277, 898, 1686]),\n",
       "       list([1, 11300, 390, 1351, 9, 4, 118, 390, 7, 11300, 45, 61, 514, 390, 7, 11300]),\n",
       "       list([1, 11300, 390, 1351, 9, 4, 118, 390, 7, 11300, 45, 61, 514, 390, 7, 11300]),\n",
       "       list([1, 931, 14, 20, 9, 1167, 9, 394, 55, 6415, 78, 2956, 963, 458, 24, 168]),\n",
       "       list([1, 57, 931, 379, 20, 116, 856, 42, 433, 881, 57, 281, 33, 32, 1771, 12])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> i wouldn't rent this one even on dollar rental night\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把id序列转化为相应的字符串\n",
    "def decode_review(text):\n",
    "    return ' '.join([id2word_new[i] for i in text])\n",
    "\n",
    "# 显示其中一个评价\n",
    "decode_review(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的id2word_new不用于训练模型，只是为了加深对该数据集的理解，并且能将原来的id列表转换成原来的句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下面开始构建用于训练的词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将中间结果保存到本地，万一程序崩了还得重玩，保存的是文本数据，不是ID\n",
    "def write_file(f_path, xs, ys):\n",
    "    with open(f_path, 'w', encoding='utf-8') as f:\n",
    "        for x, y in zip(xs, ys):\n",
    "            \n",
    "            # 将id转换成对应的单词，然后将字符串写入到硬盘\n",
    "            # 每一行的第一个数字是标签值，然后加一个\\t字符\n",
    "            # 后跟每一条影评的句子(切片操作从1开始是为了去掉开始字符)\n",
    "            # 现在每一条在train.txt和test.txt中的数据如下所示：\n",
    "            # 0\tyou'd better choose paul verhoeven's even if you have watched it\n",
    "            # 0\tming the merciless does a little bardwork and a movie most foul\n",
    "            f.write(str(y)+ '\\t' +' '.join([id2word_new[i] for i in x][1:]) + '\\n')\n",
    "\n",
    "write_file('./data/train.txt', x_train, y_train)\n",
    "write_file('./data/test.txt', x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建语料表，基于词频来进行统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 336148),\n",
       " ('and', 164097),\n",
       " ('a', 163040),\n",
       " ('of', 145847),\n",
       " ('to', 135708),\n",
       " ('is', 107313),\n",
       " ('br', 101871),\n",
       " ('in', 93934),\n",
       " ('it', 79060),\n",
       " ('i', 77142)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter()\n",
    "\n",
    "with open('./data/train.txt', encoding='utf-8') as f:\n",
    "    text_list = f.readlines()\n",
    "    for line in text_list:\n",
    "        line = line.strip()\n",
    "        label, words = line.split('\\t')\n",
    "        words = words.split(' ')\n",
    "        \n",
    "        # Counter的update方法可以是序列类型，\n",
    "        # 如果要更新的关键字已存在，则对它的值进行求和；如果不存在，则添加\n",
    "        counter.update(words)\n",
    "\n",
    "# most_common()方法如果不传参数表示考虑所有的元素\n",
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20598"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 只考虑频率大于10的词\n",
    "words = ['<pad>'] + [w for w, freq in counter.most_common() if freq >= 10]\n",
    "\n",
    "# 最后剩下20598个词\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('./vocab').mkdir(exist_ok=True)\n",
    "\n",
    "# 保存到硬盘\n",
    "with open('./vocab/word.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in words: f.write(w + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 现在得到新的word2id映射表，和上面的word2id是不一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<pad>', 0),\n",
       " ('the', 1),\n",
       " ('and', 2),\n",
       " ('a', 3),\n",
       " ('of', 4),\n",
       " ('to', 5),\n",
       " ('is', 6),\n",
       " ('br', 7),\n",
       " ('in', 8),\n",
       " ('it', 9)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = dict()\n",
    "\n",
    "# 读入刚才制作的映射表\n",
    "with open('./vocab/word.txt', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        word2idx[line] = i\n",
    "        \n",
    "list(word2idx.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding层\n",
    "- 可以基于网络来训练，也可以直接加载别人训练好的，一般都是加载预训练模型\n",
    "- 这里有一些常用的：https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![title](./img/SentimentAnalysis3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- At line 0\n",
      "- At line 100000\n",
      "- At line 200000\n",
      "- At line 300000\n"
     ]
    }
   ],
   "source": [
    "# embedding是所有词的词向量表，每一行表示一个词向量\n",
    "# 里面有20598个不同的词，再加一个unknown，20599*50\n",
    "# 初始化一个全0矩阵，+1表示第20598行对应的向量表示unknown\n",
    "embedding = np.zeros((len(word2idx) + 1, 50)) \n",
    "\n",
    "# 使用别人已经预训练好的词向量，读入文件glove.6B.50d.txt，\n",
    "# 里面包含40万个词的词向量，每个词向量的长度是50\n",
    "with open('./data/glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "    \n",
    "    # count统计在词典中的词的数量\n",
    "    count = 0\n",
    "    for i, line in enumerate(f):\n",
    "        if i % 100000 == 0: print('- At line {}'.format(i)) #打印处理了多少数据\n",
    "        \n",
    "        # 每一行文本的格式如下所示\n",
    "        # the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688...\n",
    "        line = line.strip()\n",
    "        sp = line.split(' ')\n",
    "        \n",
    "        # vec表示该词的向量\n",
    "        word, vec = sp[0], sp[1:]\n",
    "        \n",
    "        # 如果一个词在我们定义的词典中，就将对应下标的向量转换成已经训练好的向量\n",
    "        if word in word2idx:\n",
    "            count += 1\n",
    "            embedding[word2idx[word]] = np.array(vec, dtype='float32') # 将词转换成对应的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在已经得到每个词索引所对应的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19676 / 20598] words have found pre-trained values\n",
      "Saved ./vocab/word.npy\n"
     ]
    }
   ],
   "source": [
    "print(\"[%d / %d] words have found pre-trained values\"%(count, len(word2idx)))\n",
    "\n",
    "# embedding是所有词的词向量表，每一行表示一个词向量\n",
    "# 保存得到的词向量矩阵，避免重复处理数据\n",
    "np.save('./vocab/word.npy', embedding)\n",
    "print('Saved ./vocab/word.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建训练数据\n",
    "\n",
    "- 注意所有的输入样本必须都是相同shape(文本长度，词向量维度等)，如果一篇文档过长就将其截断，过短则补齐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据生成器\n",
    "- tf.data.Dataset.from_tensor_slices(tensor)：将tensor沿其第一个维度切片，返回一个含有N个样本的数据集，这样做的问题就是需要将整个数据集整体传入，然后切片建立数据集类对象，比较占内存。\n",
    "\n",
    "- tf.data.Dataset.from_generator(data_generator,output_data_type,output_data_shape)：从一个生成器中不断读取样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义生成器函数\n",
    "def data_generator(f_path, params):\n",
    "    \n",
    "    with open(f_path, encoding='utf-8') as f:\n",
    "        \n",
    "        print('Reading', f_path)\n",
    "        \n",
    "        for line in f:\n",
    "            \n",
    "            # 每一行文本的格式如下：\n",
    "            # 0 this movie is terrible but it has some good effects\n",
    "            # 第一个数字是标签值，后接一个\\t字符\n",
    "                \n",
    "            line = line.strip()\n",
    "            label, text = line.split('\\t')\n",
    "            text = text.split(' ')\n",
    "            \n",
    "            # 将句子中的每个词转换成id\n",
    "            # params是随后定义的一个字典，保存了所有的参数\n",
    "            # 如果词不在字典中，则用len(word2idx)代替(即unknown对应的id)\n",
    "            x = [params['word2idx'].get(w, len(word2idx)) for w in text]\n",
    "            \n",
    "            # params['max_len']指定了句子的最大长度，不足则补齐，超过则截断\n",
    "            if len(x) >= params['max_len']: x = x[:params['max_len']] # 截断操作\n",
    "            else: x += [0] * (params['max_len'] - len(x))  # 补齐操作\n",
    "                \n",
    "            y = int(label)\n",
    "            \n",
    "            # yield关键字的用法请看其他笔记，简单来说就是当程序执行到yield关键字时，\n",
    "            # 会将x，y返回，这里就可以看成是返回以一个样本数据，x是特征，y是标签\n",
    "            \n",
    "            # 当下一次调用data_generator()函数时，函数不会重头开始执行，而是从上一次\n",
    "            # 遇到yield关键字的地方开始执行，这里也就是回到for循环的开始位置\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_training参数表示是从训练集中取数据还是测试集中取数据\n",
    "def dataset(is_training, params):\n",
    "    \n",
    "    # ()表示是一个标量，也就是一个数字\n",
    "    _shapes = ((params['max_len'],), ())\n",
    "    _types = (tf.int32, tf.int32)\n",
    "      \n",
    "    # 参数output_shapes和output_types的类型分别是\n",
    "    # tf.TensorShape和tf.dtypes.DType\n",
    "    # 用于指定张量的形状和类型，data_generator()函数返回两个值，\n",
    "    # 因此_shapes和_types需要有两个元素，分别对应形状和类型\n",
    "    \n",
    "    # 从训练集取数据\n",
    "    if is_training:\n",
    "        # from_generator()方法返回一个dataset对象\n",
    "        # 注意这里的lambda函数没有输入参数，所以直接跟冒号\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            lambda : data_generator(params['train_path'], params),\n",
    "            output_shapes = _shapes,\n",
    "            output_types = _types,)\n",
    "        \n",
    "        ds = ds.shuffle(params['num_samples'])\n",
    "        ds = ds.batch(params['batch_size'])\n",
    "        \n",
    "        # 设置缓存序列，根据可用的CPU动态设置并行调用的数量，说白了就是加速\n",
    "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    # 从测试集取数据\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            lambda : data_generator(params['test_path'], params),\n",
    "            output_shapes = _shapes,\n",
    "            output_types = _types,)\n",
    "        ds = ds.batch(params['batch_size'])\n",
    "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义网络模型\n",
    "- 定义好都有哪些层\n",
    "- 前向传播走一遍就行了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding_lookup的作用："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/SentimentAnalysis5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/SentimentAnalysis16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义模型类，继承tf.keras.Model\n",
    "# 必须重写__init__方法和call()方法\n",
    "class Model(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # 下面这些都是类的属性，到时候可以直接通过属性调用\n",
    "        \n",
    "        # 将词向量矩阵构建成Variable类型\n",
    "        # embedding是所有词的词向量表，每一行表示一个词向量\n",
    "        self.embedding = tf.Variable(np.load('./vocab/word.npy'),\n",
    "                                     dtype=tf.float32,\n",
    "                                     name='pretrained_embedding',\n",
    "                                     trainable=False)\n",
    "        \n",
    "        self.drop1 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.drop2 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.drop3 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        \n",
    "        # 将LSTM包装在Bidirectional层中\n",
    "        # 现在相当于每层RNN有两层LSTM，一层正向，一层反向\n",
    "        # 正向层和反向层不共享参数\n",
    "        \n",
    "        # 第一个参数表示通过LSTM层后得到的特征个数(实际上是sigmoid层神经元的个数)\n",
    "        # 这个参数决定了Cell_state和hidden_state向量的维数\n",
    "        \n",
    "        # return_sequences表示是否返回所有序列，\n",
    "        # 如果为Flase则只返回最后 一个时刻的输出值，默认为Flase\n",
    "        self.rnn1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "            params['rnn_units'], return_sequences=True))\n",
    "        \n",
    "        self.rnn2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "            params['rnn_units'], return_sequences=True))\n",
    "        \n",
    "        # 最后一层rnn只返回最后一个时刻的输出值\n",
    "        self.rnn3 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "            params['rnn_units'], return_sequences=False))\n",
    "\n",
    "        self.drop_fc = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        \n",
    "        # 由于是双向循环网络，前向层和后向层的特征会拼接在一起，因此全连接层\n",
    "        # 神经元的个数应该是两倍\n",
    "        self.fc = tf.keras.layers.Dense(2*params['rnn_units'], activation=\"relu\")\n",
    "\n",
    "        self.out_linear = tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "\n",
    "  \n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        # 如果张量的元素类型不是整型，转换成整型\n",
    "        if inputs.dtype != tf.int32: inputs = tf.cast(inputs, tf.int32)\n",
    "        \n",
    "        # 在embedding中查找词向量，后续的代码中传进来的inputs就是一个句子，\n",
    "        # 句子中的词都用id表示，例如：[1, 2, 3, 4, 5, ..., 100]\n",
    "        x = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "        \n",
    "        # 这里其实是按照处理顺序一步步的处理了数据，并没有使用sequential模型 \n",
    "        \n",
    "        # 开始搭建网络模型，使用从embedding中查找出的词向量\n",
    "        # 作为输入，通过dropout层的数据形状不变，但是部分元素会变成0\n",
    "        x = self.drop1(x, training=training)\n",
    "        \n",
    "        # 通过rnn层后，词向量的维度会变化成指定的神经元的个数\n",
    "        x = self.rnn1(x)\n",
    "\n",
    "        x = self.drop2(x, training=training)\n",
    "        \n",
    "        x = self.rnn2(x)\n",
    "\n",
    "        x = self.drop3(x, training=training)\n",
    "        \n",
    "        x = self.rnn3(x)\n",
    "\n",
    "        x = self.drop_fc(x, training=training)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = self.out_linear(x)\n",
    "        \n",
    "        # 返回最后的输出值，即属于两个类别的概率\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  'vocab_path': './vocab/word.txt',\n",
    "  'train_path': './data/train.txt',\n",
    "  'test_path': './data/test.txt',\n",
    "  'num_samples': 5000,\n",
    "  'num_labels': 2,\n",
    "  'batch_size': 64,\n",
    "  'max_len': 300, # 句子的最大长度\n",
    "  'rnn_units': 128,\n",
    "  'dropout_rate': 0.2,\n",
    "  'num_patience': 3, # 容忍次数\n",
    "  'lr': 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用来判断迭代何时停止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里使用了函数注解，参数后面的冒号表示该参数希望传入的类型\n",
    "def is_descending(history:list):\n",
    "    \n",
    "    history = history[-(params['num_patience']+1):]\n",
    "    \n",
    "    for i in range(1, len(history)):\n",
    "        if history[i-1] <= history[i]:\n",
    "            return False\n",
    "        \n",
    "    return True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "\n",
    "# 读入word2id，其实可以使用json格式保存到硬盘\n",
    "# 但是之前使用的是txt文件\n",
    "with open(params['vocab_path'], encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        word2idx[line] = i\n",
    "\n",
    "# 添加两个参数\n",
    "params['word2idx'] = word2idx\n",
    "params['vocab_size'] = len(word2idx) + 1\n",
    "\n",
    "\n",
    "# 这里的Model是刚才自定义的Model对象:\n",
    "# class Model(tf.keras.Model):\n",
    "model = Model(params)\n",
    "\n",
    "# #设置输入的大小，或者fit时候也能自动找到\n",
    "# model.build(input_shape=(None, None))\n",
    "\n",
    "# return initial_learning_rate * decay_rate ^ (step / decay_steps)\n",
    "# 相当于加了一个指数衰减函数\n",
    "decay_lr = tf.optimizers.schedules.ExponentialDecay(params['lr'], \n",
    "                    decay_steps=1000, decay_rate=0.95)\n",
    "\n",
    "optim = tf.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "# 保存历史准确率\n",
    "history_acc = []\n",
    "\n",
    "best_acc = 0.0\n",
    "\n",
    "t0 = time.time()\n",
    "logger = logging.getLogger('tensorflow')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optim的lr属性保存了优化器的学习率，这个学习率可以是一个\n",
    "# 标量，也可以是一个优化对象，具体查看API\n",
    "optim.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./data/train.txt\n",
      "INFO:tensorflow:Step 0 | Loss: 0.6940 | Spent: 3.1 secs | LR: 0.000100\n",
      "INFO:tensorflow:Step 50 | Loss: 0.6747 | Spent: 8.4 secs | LR: 0.000100\n",
      "INFO:tensorflow:Step 100 | Loss: 0.6700 | Spent: 8.0 secs | LR: 0.000099\n",
      "INFO:tensorflow:Step 150 | Loss: 0.6370 | Spent: 8.6 secs | LR: 0.000099\n",
      "INFO:tensorflow:Step 200 | Loss: 0.6402 | Spent: 8.2 secs | LR: 0.000099\n",
      "INFO:tensorflow:Step 250 | Loss: 0.8351 | Spent: 8.7 secs | LR: 0.000099\n",
      "INFO:tensorflow:Step 300 | Loss: 0.6316 | Spent: 9.2 secs | LR: 0.000098\n",
      "INFO:tensorflow:Step 350 | Loss: 0.5982 | Spent: 8.6 secs | LR: 0.000098\n",
      "Reading ./data/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.682\n",
      "INFO:tensorflow:Best Accuracy: 0.682\n",
      "Reading ./data/train.txt\n",
      "INFO:tensorflow:Step 400 | Loss: 0.6197 | Spent: 32.5 secs | LR: 0.000098\n",
      "INFO:tensorflow:Step 450 | Loss: 0.5912 | Spent: 7.0 secs | LR: 0.000098\n",
      "INFO:tensorflow:Step 500 | Loss: 0.5484 | Spent: 6.7 secs | LR: 0.000097\n",
      "INFO:tensorflow:Step 550 | Loss: 0.5383 | Spent: 6.7 secs | LR: 0.000097\n",
      "INFO:tensorflow:Step 600 | Loss: 0.6092 | Spent: 6.8 secs | LR: 0.000097\n",
      "INFO:tensorflow:Step 650 | Loss: 0.5158 | Spent: 6.8 secs | LR: 0.000097\n",
      "INFO:tensorflow:Step 700 | Loss: 0.5642 | Spent: 7.0 secs | LR: 0.000096\n",
      "INFO:tensorflow:Step 750 | Loss: 0.6006 | Spent: 6.4 secs | LR: 0.000096\n",
      "Reading ./data/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.713\n",
      "INFO:tensorflow:Best Accuracy: 0.713\n",
      "Reading ./data/train.txt\n",
      "INFO:tensorflow:Step 800 | Loss: 0.6457 | Spent: 38.4 secs | LR: 0.000096\n",
      "INFO:tensorflow:Step 850 | Loss: 0.5403 | Spent: 8.8 secs | LR: 0.000096\n",
      "INFO:tensorflow:Step 900 | Loss: 0.5536 | Spent: 8.6 secs | LR: 0.000095\n",
      "INFO:tensorflow:Step 950 | Loss: 0.6263 | Spent: 8.6 secs | LR: 0.000095\n",
      "INFO:tensorflow:Step 1000 | Loss: 0.6413 | Spent: 8.7 secs | LR: 0.000095\n",
      "INFO:tensorflow:Step 1050 | Loss: 0.6240 | Spent: 8.9 secs | LR: 0.000095\n",
      "INFO:tensorflow:Step 1100 | Loss: 0.5541 | Spent: 8.9 secs | LR: 0.000095\n",
      "INFO:tensorflow:Step 1150 | Loss: 0.6083 | Spent: 8.0 secs | LR: 0.000094\n",
      "Reading ./data/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.734\n",
      "INFO:tensorflow:Best Accuracy: 0.734\n",
      "Reading ./data/train.txt\n",
      "INFO:tensorflow:Step 1200 | Loss: 0.5423 | Spent: 39.1 secs | LR: 0.000094\n",
      "INFO:tensorflow:Step 1250 | Loss: 0.5503 | Spent: 8.7 secs | LR: 0.000094\n",
      "INFO:tensorflow:Step 1300 | Loss: 0.5424 | Spent: 9.0 secs | LR: 0.000094\n",
      "INFO:tensorflow:Step 1350 | Loss: 0.6081 | Spent: 8.7 secs | LR: 0.000093\n",
      "INFO:tensorflow:Step 1400 | Loss: 0.6658 | Spent: 8.8 secs | LR: 0.000093\n",
      "INFO:tensorflow:Step 1450 | Loss: 0.5635 | Spent: 9.0 secs | LR: 0.000093\n",
      "INFO:tensorflow:Step 1500 | Loss: 0.6247 | Spent: 8.8 secs | LR: 0.000093\n",
      "INFO:tensorflow:Step 1550 | Loss: 0.6185 | Spent: 8.3 secs | LR: 0.000092\n",
      "Reading ./data/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.731\n",
      "INFO:tensorflow:Best Accuracy: 0.734\n",
      "Reading ./data/train.txt\n",
      "INFO:tensorflow:Step 1600 | Loss: 0.4694 | Spent: 39.2 secs | LR: 0.000092\n",
      "INFO:tensorflow:Step 1650 | Loss: 0.4962 | Spent: 8.7 secs | LR: 0.000092\n",
      "INFO:tensorflow:Step 1700 | Loss: 0.5821 | Spent: 8.6 secs | LR: 0.000092\n",
      "INFO:tensorflow:Step 1750 | Loss: 0.5614 | Spent: 9.7 secs | LR: 0.000091\n",
      "INFO:tensorflow:Step 1800 | Loss: 0.5217 | Spent: 8.7 secs | LR: 0.000091\n",
      "INFO:tensorflow:Step 1850 | Loss: 0.5688 | Spent: 9.0 secs | LR: 0.000091\n",
      "INFO:tensorflow:Step 1900 | Loss: 0.6250 | Spent: 8.6 secs | LR: 0.000091\n",
      "INFO:tensorflow:Step 1950 | Loss: 0.5140 | Spent: 8.1 secs | LR: 0.000090\n",
      "Reading ./data/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.721\n",
      "INFO:tensorflow:Best Accuracy: 0.734\n",
      "Reading ./data/train.txt\n",
      "INFO:tensorflow:Step 2000 | Loss: 0.5286 | Spent: 38.9 secs | LR: 0.000090\n",
      "INFO:tensorflow:Step 2050 | Loss: 0.5574 | Spent: 8.6 secs | LR: 0.000090\n",
      "INFO:tensorflow:Step 2100 | Loss: 0.5682 | Spent: 8.7 secs | LR: 0.000090\n",
      "INFO:tensorflow:Step 2150 | Loss: 0.6539 | Spent: 8.7 secs | LR: 0.000090\n",
      "INFO:tensorflow:Step 2200 | Loss: 0.7789 | Spent: 8.8 secs | LR: 0.000089\n",
      "INFO:tensorflow:Step 2250 | Loss: 0.6929 | Spent: 9.0 secs | LR: 0.000089\n",
      "INFO:tensorflow:Step 2300 | Loss: 0.6283 | Spent: 8.4 secs | LR: 0.000089\n",
      "Reading ./data/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.709\n",
      "INFO:tensorflow:Best Accuracy: 0.734\n",
      "INFO:tensorflow:Testing Accuracy not improved over 3 epochs, Early Stop\n"
     ]
    }
   ],
   "source": [
    "# 可以自己定义终止训练的条件\n",
    "while best_acc == 0 or best_acc < 0.8:\n",
    "    \n",
    "    \n",
    "    # 从数据生成器中取数据训练模型，Dataset对象中每个元素都是一个二元组\n",
    "    # 格式类似于:([1, 2, 3, 4, 5, ..., 100], 1)，第一个元素是句子，第二个元素是标签值\n",
    "    for texts, labels in dataset(is_training=True, params=params):\n",
    "        \n",
    "        # 梯度带，监视所有在上下文中的变量\n",
    "        # 然后可以通过调用.gradient()获得任何上下文中计算得出的张量的梯度\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # 由于继承了tf.keras.model对象，\n",
    "            # 这里其实相当于调用model对象的call()方法\n",
    "            # 拿到logits相当于完成了前向传播过程\n",
    "            logits = model(texts, training=True)\n",
    "            \n",
    "            # 定义损失值\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "            \n",
    "            # 取平均值\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        # 下面进行反向传播，指定新的学习率\n",
    "        optim.lr.assign(decay_lr(global_step))\n",
    "        \n",
    "        # trainable_variables属性保存了模型中所有的可训练变量\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "\n",
    "        # 利用刚才计算得到的梯度，使用优化器更新参数\n",
    "        # 这里其实也可以调用optim.minimize()方法，请参考API\n",
    "        optim.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if global_step % 50 == 0:\n",
    "            logger.info(\"Step {} | Loss: {:.4f} | Spent: {:.1f} secs | LR: {:.6f}\".format(\n",
    "                global_step, loss.numpy().item(), time.time()-t0, optim.lr.numpy().item()))\n",
    "            t0 = time.time()\n",
    "            \n",
    "        global_step += 1\n",
    "\n",
    "    m = tf.keras.metrics.Accuracy()\n",
    "    \n",
    "    # 从测试集中取数据\n",
    "    for texts, labels in dataset(is_training=False, params=params):\n",
    "        \n",
    "        logits = model(texts, training=False)\n",
    "        y_pred = tf.argmax(logits, axis=-1)\n",
    "        \n",
    "        # 计算准确率\n",
    "        m.update_state(y_true=labels, y_pred=y_pred)\n",
    "    \n",
    "    # 返回结果\n",
    "    acc = m.result().numpy()\n",
    "    \n",
    "    logger.info(\"Evaluation: Testing Accuracy: {:.3f}\".format(acc))\n",
    "    \n",
    "    history_acc.append(acc)\n",
    "  \n",
    "    if acc > best_acc: best_acc = acc\n",
    "        \n",
    "    logger.info(\"Best Accuracy: {:.3f}\".format(best_acc))\n",
    "  \n",
    "    if len(history_acc) > params['num_patience'] and is_descending(history_acc):\n",
    "        logger.info(\"Testing Accuracy not improved over {} epochs, Early Stop\".format(params['num_patience']))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow26] *",
   "language": "python",
   "name": "conda-env-tensorflow26-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
