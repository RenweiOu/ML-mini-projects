#  Coursework 2: Support Vector Machines

> Q1. Write down the first 7 digits of your student ID as $s1s2 s3 s4 s5 s6 s7$.

The first 7 digits of my student ID are 2, 1, 0, 2, 3, 1, 0.

> Q2. Find R1 which is the remainder of $\frac{s1+s2+s3+s4+s5+s6+s7}{4}$.

The R1 value of my student ID is 1, according to Table 1, so I used one-against-all method to design my multi-class SVM classifier.

> Q3. Create a linearly separable two-dimensional dataset of your own, which consists of
3 classes.

The linearly separable dataset I created is as follow, each class include 10 samples:

<center>

| Sample of Class 1 | Sample of Class 2 | Sample of Class 3 |
| :----: |  :----: |  :----: |
| $x_1$ = [-0.9, -1.3] | $x_{11}$ = [-0.4,  1.0] | $x_{21}$ = [0.9, -0.8] |
| $x_2$ = [-1.2, -1.0] | $x_{12}$ = [-0.1, 0.9] | $x_{22}$ = [1.3, -1.1] |
| $x_3$ = [-1.5, -0.6] | $x_{13}$ = [0.0, 1.1] | $x_{23}$ = [1.1, -0.7] |
| $x_4$ = [-1.4, -1.2] | $x_{14}$ = [0.1,  0.8] | $x_{24}$ = [1.2, -1.0] |
| $x_5$ = [-1.1, -1.0] | $x_{15}$ = [-0.2,  0.7] | $x_{25}$ = [1.4, -1.1] |
| $x_6$ = [-1.3, -1.2] | $x_{16}$ = [-0.1,  1.6] | $x_{26}$ = [0.9, -1.0] |
| $x_7$ = [-0.9, -1.1] | $x_{17}$ = [0.1,  1.3] | $x_{27}$ = [0.8, -1.0] |
| $x_8$ = [-0.9, -0.4] | $x_{18}$ = [-0.2,  1.0] | $x_{28}$ = [1.1, -1.5] |
| $x_9$ = [-1.0, -1.3] | $x_{19}$ = [-0.6,  1.0] | $x_{29}$ = [0.8, -0.9] |
| $x_{10}$ = [-1.0 , -0.7] | $x_{20}$ = [0.1,  0.8] | $x_{30}$ = [0.5, -0.6] |

</center>

> Q4. Plot the dataset in Q3 to show that the samples are linearly separable. Explain
why your dataset is linearly separable.

The figure contains three classes samples in Q3 is as follow:

<center>

<img src="fig1.png" width=500>

</center>

It's obvious that this dataset is linearly separable, where the blue, red, green points in the figure represent the samples of Class 1, Class 2 and Class 3 respectively. We see that line 1 can classify Class 1 and not Class 1 (Class 2 or Class 3), line 2 can classify Class 2 and not Class 2, line 3 can classify Class 3 and not Class 3.

> Q5. According to the method obtained in Q2, draw a block diagram at SVM level to
show the structure of the multi-class classifier constructed by linear SVMs. Explain
the design (e.g., number of inputs, number of outputs, number of SVMs used, class
label assignment, etc.) and describe how this multi-class classifier works.

According to the one-against-all combining method, I need combine 3 SVMs (SVM-1, SVM-2 and SVM-3) to construct a multi-class SVM classifier. 

That multi-calss SVM classifier make final desision for one samples by using majority vote, that is, each 2-class SVM vote for each class that the sample may belongs to, and the class that has the most vote is the final desision of multi-class SVM classifier. 

>Q6. According to your dataset in Q3 and the design of your multi-class classifier in Q5,
identify the support vectors of the linear SVMs by “inspection” and design their
hyperplanes by hand. Show the calculations and explain the details of your design.

Take the design process of SVM-1 that can classify the samples belongs to class 1 or not class 1 as example:

Firstly, by inspection, we can observe that the samples $x_8$ = [-0.9, -0.4], $x_{19}$ = [-0.6,  1.0] and $x_{28}$ = [1.1, -1.5] are support vectors. We define the labels of samples belonging to class 1 as +1, and not class 1 as -1, so we have $y_8$ = 1 and $y_{19}$ = -1, $y_{28}$ = -1.

<center>

<img src="fig2.png" width=500>

</center>

Now we need to find the hyperplane $xw + w_0 = 0$ that can classify the samples belonging to class 1 or not class 1 for SVM-1, we know that: 

$
\begin{align*}
w = \sum_{i = 1}^{N}\lambda_iy_ix_i^T
\end{align*}
$


Due to the $\lambda_i$ of the samples that are not support vectors are equal to 0, so we have:

<center>

$
\begin{align*}
w & = \sum_{i = 1}^{N}\lambda_iy_ix_i^T \\
& = \lambda_8y_8x_8^T + \lambda_{19}y_{19}x_{19}^T + \lambda_{28}y_{28}x_{28}^T \\
& =  \lambda_8\begin{bmatrix}
-0.9 \\
-0.4
\end{bmatrix} - \lambda_{19}\begin{bmatrix}
-0.6 \\
1.0
\end{bmatrix} -  \lambda_{28}\begin{bmatrix}
1.1 \\
-1.5
\end{bmatrix}\\
& =  \begin{bmatrix}
-0.9\lambda_8 + 0.6\lambda_{19} - 1.1\lambda_{28}\\
-0.4\lambda_8 - \lambda_{19} + 1.5\lambda_{28}
\end{bmatrix}
\end{align*}
$

</center>

We know that the equation $y_i(xw + w_0) = 1$ is hold when x is support vector, so we have:

$
y_8(x_8w + w_0) = ([-0.9, -0.4]) \begin{bmatrix}
-0.9\lambda_8 + 0.6\lambda_{19} - 1.1\lambda_{28}\\
-0.4\lambda_8 - \lambda_{19} + 1.5\lambda_{28}
\end{bmatrix} + w_0) = 1 
$

$
y_{19}(x_{19}w + w_0) = -([-0.6, 1.0]) \begin{bmatrix}
-0.9\lambda_8 + 0.6\lambda_{19} - 1.1\lambda_{28}\\
-0.4\lambda_8 - \lambda_{19} + 1.5\lambda_{28}
\end{bmatrix} + w_0) = 1
$

$
y_{28}(x_{28}w + w_0) = -([1.1, -1.5]) \begin{bmatrix}
-0.9\lambda_8 + 0.6\lambda_{19} - 1.1\lambda_{28}\\
-0.4\lambda_8 - \lambda_{19} + 1.5\lambda_{28}
\end{bmatrix} + w_0) = 1
$

We sort the equations above and get the equations following:

$
0.97\lambda_8 - 0.14\lambda_{19} + 0.39\lambda_{28} + w_0 = 1 \tag{1}
$

$
0.14\lambda_8 - 1.36\lambda_{19} + 2.16\lambda_{28} + w_0 = -1 \tag{2}
$

$
-0.39\lambda_8 + 2.16\lambda_{19} - 3.46\lambda_{28} + w_0 = -1 \tag{3}
$

Another satisfied equation is that:

<center>

$\sum_{i = 1}^N \lambda_iy_i = \lambda_{8} - \lambda_{19} - \lambda_{28} = 0
\tag{4}
$

</center>

Combine the euquations (1), (2), (3) and (4), now we have the following equations:

$
\begin{bmatrix}
0.97 & -0.14 & 0.39 & 1 \\
0.14 & -1.36 & 2.16 & 1 \\
-0.39 & 2.16 & -3.46 & 1 \\
1 & -1 & -1 & 0 \\
\end{bmatrix}\begin{bmatrix}
\lambda_{8} \\
\lambda_{19} \\
\lambda_{28} \\
w_0 \\
\end{bmatrix} = \begin{bmatrix}
1 \\
-1 \\
-1 \\
0 \\
\end{bmatrix} 
$

We solve the equations above, and get:

$
\begin{bmatrix}
\lambda_{8} \\
\lambda_{19} \\
\lambda_{28} \\
w_0 \\
\end{bmatrix} = \begin{bmatrix}
1.87 \\
1.26 \\
0.61 \\
-0.87 \\
\end{bmatrix} 
$


Finally, we employed the results above to find the $w$:

$
\begin{align*}
w & = \sum_{i = 1}^{N}\lambda_iy_ix_i^T \\
& = \lambda_8y_8x_8^T + \lambda_{19}y_{19}x_{19}^T + \lambda_{28}y_{28}x_{28}^T \\
& =  \lambda_8\begin{bmatrix}
-0.9 \\
-0.4
\end{bmatrix} - \lambda_{19}\begin{bmatrix}
-0.6 \\
1.0
\end{bmatrix} -  \lambda_{28}\begin{bmatrix}
1.1 \\
-1.5
\end{bmatrix}\\
& =  \begin{bmatrix}
-0.9\lambda_8 + 0.6\lambda_{19} - 1.1\lambda_{28}\\
-0.4\lambda_8 - \lambda_{19} + 1.5\lambda_{28}
\end{bmatrix} \\
& = \begin{bmatrix}
-1.60 \\
-1.09
\end{bmatrix}
\end{align*}
$

and the hyperplane of SVM-1 is:

$x\begin{bmatrix}
-1.60 \\
-1.09
\end{bmatrix} - 0.87 = 0$

Similarly, we can employed the same method as SVM-1 to find the hyperplane of SVM-2 and SVM-3. For SVM-2, we observe that the support vectors are $x_8$ = [-0.9, -0.4], $x_{15}$ = [-0.2,  0.7] and $x_{30}$ = [0.5, -0.6], as shown in the following figure:

<center>

<img src="fig3.png" width=500>

</center>

We next repeat the same procedure in SVM-1, and finally we get

$w=\begin{bmatrix}
0.24 \\
1.67
\end{bmatrix} $ 

<center>

$w_0 = -0.12$

</center>

for SVM-2, so the hyperplane of SVM-2 is:

$x\begin{bmatrix}
0.24 \\
1.67
\end{bmatrix} - 0.12 = 0$

For SVM-3, we observe that the support vectors are $x_1$ = [-0.9, -1.3], $x_{20}$ = [0.1,  0.8] and $x_{30}$ = [0.5, -0.6], as shown in the following figure:

<center>

<img src="fig4.png" width=500>

</center>

The hyperplane of SVM-3 is:

$x\begin{bmatrix}
1.87 \\
-0.89
\end{bmatrix} - 0.47 = 0$

Now we get the decision function of each SVMs: 

$f_1(x) = sign(x\begin{bmatrix}
-1.60 \\
-1.09
\end{bmatrix} - 0.87)$

$f_2(x) = sign(x\begin{bmatrix}
0.24 \\
1.67
\end{bmatrix} - 0.12)$

$f_3(x) = sign(x\begin{bmatrix}
1.87 \\
-0.89
\end{bmatrix} - 0.47)$

Next we can use these decision functions for sample prediction.


> Q7. Produce a test dataset by averaging the samples for each row in Table 2, i.e., (sample of class 1 + sample of class 2 + sample of class 3)/3. Summarise the results in the form of Table 3, where N is the number of SVMs in your design and “Classification” is the class determined by your multi-class classifier. Explain how to get the “Classification” column using one test sample. Show the calculations for one or two samples to demonstrate how to get the contents in the table.

The distribution of test samples is shown in the following figure:


The classification results of each test sample for my multi-class SVM are shown in the following table:


| Test Sample | Output of SVM 1 | Output of SVM 2 | Output of SVM 3 | Classification |
| :----: | :----: |  :----: |  :----: | :----: |
| [-0.1, -0.4] | -1 | -1 | -1 | Class 2 |
| [0.0 , -0.4] | -1 | -1 | -1 | Class 2 |
| [-0.1, -0.1] | -1 | -1 | -1 | Class 2 |
| [0.0 , -0.5] | -1 | -1 | -1 | Class 2 |
| [0.0 , -0.5] | -1 | -1 | -1 | Class 2 |
| [-0.2, -0.2] | -1 | -1 | -1 | Class 2 |
| [ 0.0 , -0.3] | -1 | -1 | -1 | Class 2 |
| [ 0.0 , -0.3] | -1 | -1 | -1 | Class 2 |
| [-0.3, -0.4] | 1 | -1 | -1 | Class 2 |
| [-0.1, -0.2] | -1 | -1 | -1 | Class 2 |



For test sample $x=[-0.3, -0.4]$, we substitude that value into the $f_1(x)$, $f_2(x)$, $f_3(x)$ respectively, we get:

$f_1([-0.3, -0.4]) = sign([-0.3, -0.4]\begin{bmatrix}
-1.60 \\
-1.09
\end{bmatrix} - 0.87) = sign(0.04) = 1$

$f_2([-0.3, -0.4]) = sign([-0.3, -0.4]\begin{bmatrix}
0.24 \\
1.67
\end{bmatrix} - 0.12) = sign(-0.86) = -1$

$f_3([-0.3, -0.4]) = sign([-0.3, -0.4]\begin{bmatrix}
1.87 \\
-0.89
\end{bmatrix} - 0.47) = sign(-0.68) = -1$

So the outputs of SVM 1, SVM 2, SVM 3 are 1, -1, -1 respectively, it means that SVM 1 classifies that sample $x=[-0.3, -0.4]$ into class 1 and SVM 2 and SVM 3 consider that sample should not be class 2 (class 1 or class 3) and class 3 (class 1 or class 2). According to the principle of yielding the final decision based on One-against-All approach, class 1 obtain tha most votes among all the SVMs, therefore the multi-class SVM classifier classify that sample into class 1.

